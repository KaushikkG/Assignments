val emp=sc.textFile("/home/hadoop/YARNBOX/WORKSPACES/Sept2016WEBatch_GitRepo/SET2/MapReduceCode/Joins-MapReduce/input/emp")
val dept=sc.textFile("/home/hadoop/YARNBOX/WORKSPACES/Sept2016WEBatch_GitRepo/SET2/MapReduceCode/Joins-MapReduce/input/dept")

val ex_field=(line:String,index:Int)=> { 
val key=line.split(",")(index)
val value=(for(a <- line.split(",").zipWithIndex if a._2!=index) yield a._1).mkString(",")
(key,value)
}

val empRDD=emp.map(line=>( line.split(",")(3),line))

val deptRDD=dept.map(line=>( line.split(",")(0),line.split(",")(1)))


empRDD.join(deptRDD).map(x=> x._2).saveAsTextFile("OutFiles/joinOut")


val empDF=emp.map(line=>( line.split(",")(0), line.split(",")(1), line.split(",")(2), line.split(",")(3))).toDF("id","name","sal","deptid")

empDF.createOrReplaceTempView("emp_table")

val deptDF=dept.map(line=>( line.split(",")(0).toInt, line.split(",")(1))).toDF("deptid","deptname")

deptDF.createOrReplaceTempView("dept_table")

spark.sql("select emp_table.*,dept_table.deptname from emp_table left join dept_table on emp_table.deptid=dept_table.deptid").show

spark.sql("select emp_table.*,dept_table.deptname from emp_table left join dept_table on emp_table.deptid=dept_table.deptid").write.orc("OutFiles/joinOut")

val resOut=spark.read.orc("/home/hadoop/OutFiles/joinOut")
resOut.printSchema
resOut.show


