import org.apache.spark.SparkConf
import org.apache.spark.SparkContext


object Prog1 {
  def main(args: Array[String]): Unit = {
    
    //val conf=new SparkConf().set(key, value)
    //spark standalone mode
    val sc=new  SparkContext("local[*]","Ratings")
    
    //Create an RDD
    //UserID::MovieID::Rating::Timestamp
    val ratingsRDD=sc.textFile("file:///home/hadoop/Music/Classes/MovieLens-Work-SPARK/ml-1m/ratings.dat")
    
    /// Movie id - How many times it is rated??
    
    
    
    
  }
}