import org.apache.spark.SparkContext
import org.apache.hadoop.fs.Path


object WordCount {
  def main(args: Array[String]): Unit = {
    val sc=new SparkContext("local[*]","WordCount")
    //sc.setLogLevel(logLevel)
   val input=sc.textFile(args(0))
   val res=input.flatMap(line=>line.split(" ")).
   map(word=>(word,1)).
   reduceByKey((v1,v2)=>v1+v2)
   res.foreach(println)
   
   import org.apache.hadoop.fs._
   import org.apache.hadoop.fs.FileSystem
   import org.apache.hadoop.conf.Configuration
   val hadoopConf=new Configuration
   val fs=FileSystem.get(hadoopConf)
   if(fs.exists(new Path(args(1))
   
    
   
  }
  
}