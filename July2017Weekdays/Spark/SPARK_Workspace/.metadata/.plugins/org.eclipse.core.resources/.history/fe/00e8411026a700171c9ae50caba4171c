import org.apache.spark.sql.SparkSession
import org.apache.log4j.Logger
import org.apache.log4j.Level


object SparkSqlTest {
  def main(args: Array[String]): Unit = {
    
    Logger.getLogger("org").setLevel(Level.ERROR)
    val spark = SparkSession.builder.
    master("local").appName("Simple Application").
    getOrCreate()
    
    val empDF=spark.read.option("header","true").
    csv("/home/hadoop/YARNBOX/WORKSPACES/Sept2016WEBatch_GitRepo/SET2/MapReduceCode/Joins-MapReduce/input/emp")
    empDF.printSchema()
    
    val deptDF=spark.read.
    csv("/home/hadoop/YARNBOX/WORKSPACES/Sept2016WEBatch_GitRepo/SET2/MapReduceCode/Joins-MapReduce/input/dept").
    toDF("deptid","deptname")
    deptDF.printSchema()
    
    empDF.createOrReplaceTempView("t_emp")
    deptDF.createOrReplaceTempView("t_dept")
    
    
    
        
    
  }
}